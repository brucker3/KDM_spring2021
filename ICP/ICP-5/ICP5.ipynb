{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ICP5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNEr2TsVRxdon4pzRyHndn+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brucker3/KDM_spring2021/blob/main/ICP/ICP-5/ICP5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L68tMoXgNihH",
        "outputId": "00826bf0-fdfe-4b6f-a7b6-86da368ac28d"
      },
      "source": [
        "# intsall spark and importing necessry packages \n",
        "!pip install pyspark\n",
        "!pip install nltk\n",
        "from __future__ import print_function\n",
        "from pyspark import SparkConf, SparkContext\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import NGram\n",
        "from pyspark.ml.feature import Word2Vec\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# creating spark session\n",
        "spark = SparkSession.builder.appName(\"ICP5\").getOrCreate()\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/67/5158f846202d7f012d1c9ca21c3549a58fd3c6707ae8ee823adcaca6473c/pyspark-3.0.2.tar.gz (204.8MB)\n",
            "\u001b[K     |████████████████████████████████| 204.8MB 66kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 39.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.2-py2.py3-none-any.whl size=205186687 sha256=6cf3612c8a42828524bb5ffb45aaf9ccad7ae082d7492721bc469d4b0a02da67\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/09/da/c1f2859bcc86375dc972c5b6af4881b3603269bcc4c9be5d16\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.2\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXCY1T_7Zcr1"
      },
      "source": [
        "\n",
        "# read in of all 5 txt files \n",
        "# I tried to do this a cleaner way but spark kept skipping over some of the files \n",
        "\n",
        "txt1 = \"../content/txt1.txt\"\n",
        "txt2 = \"../content/txt2.txt\"\n",
        "txt3 = \"../content/txt3.txt\"\n",
        "txt4 = \"../content/txt4.txt\"\n",
        "txt5 = \"../content/txt5.txt\"\n",
        "\n",
        "# opening and converting read in files to use able form \n",
        "txt1 = open(txt1, \"r\")\n",
        "txt2 = open(txt2, \"r\")\n",
        "txt3 = open(txt3, \"r\")\n",
        "txt4 = open(txt4, \"r\")\n",
        "txt5 = open(txt5, \"r\")\n",
        "text1_sring = txt1.read()\n",
        "text2_sring = txt2.read()\n",
        "text3_sring = txt3.read()\n",
        "text4_sring = txt4.read()\n",
        "text5_sring = txt5.read()\n",
        "# creating spark dataframe wiht the input data. \n",
        "sentenceData = spark.createDataFrame([\n",
        "        (0, text1_sring),\n",
        "        (1, text2_sring),\n",
        "        (2, text3_sring),\n",
        "        (3, text4_sring),\n",
        "        (4, text5_sring)\n",
        "    ], [\"label\", \"sentence\"])\n",
        "\n",
        "\n",
        "word_to_vec = spark.createDataFrame([\n",
        "        ( text1_sring.split(\" \"),),\n",
        "        ( text2_sring.split(\" \"),),\n",
        "        ( text3_sring.split(\" \"),),\n",
        "        ( text4_sring.split(\" \"),),\n",
        "        ( text5_sring.split(\" \"),)\n",
        "    ], [\"text\"])\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nm35o18iiABP",
        "outputId": "360a07aa-43bb-419d-c6f7-cace2a04a99b"
      },
      "source": [
        "# tokenizer to set up for IDF calculation \n",
        "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
        "wordsData = tokenizer.transform(sentenceData)\n",
        "wordsData.show()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+--------------------+\n",
            "|label|            sentence|               words|\n",
            "+-----+--------------------+--------------------+\n",
            "|    0|When you ask Amer...|[when, you, ask, ...|\n",
            "|    1|So a new coach is...|[so, a, new, coac...|\n",
            "|    2|Whether it's in t...|[whether, it's, i...|\n",
            "|    3|This is not every...|[this, is, not, e...|\n",
            "|    4|But in 2002, when...|[but, in, 2002,, ...|\n",
            "+-----+--------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gF0CzIoND7Cm",
        "outputId": "4add1518-6c43-4dea-dde7-ec92bb5673ed"
      },
      "source": [
        "\n",
        "def TF(words):\n",
        "    # applying tf on the words data\n",
        "    # alternatively, CountVectorizer can also be used to get term frequency vectors\n",
        "    hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=10)\n",
        "    featurizedData = hashingTF.transform(words)\n",
        "    featurizedData.cache()\n",
        "    # calculating the IDF\n",
        "    idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "    idfModel = idf.fit(featurizedData)\n",
        "    rescaledData = idfModel.transform(featurizedData)\n",
        "    return rescaledData\n",
        "\n",
        "# for the orignal input \n",
        "#type(wordsData)\n",
        "wordsData.show()\n",
        "ouput = TF(wordsData)\n",
        "ouput.show()  "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+--------------------+\n",
            "|label|            sentence|               words|\n",
            "+-----+--------------------+--------------------+\n",
            "|    0|When you ask Amer...|[when, you, ask, ...|\n",
            "|    1|So a new coach is...|[so, a, new, coac...|\n",
            "|    2|Whether it's in t...|[whether, it's, i...|\n",
            "|    3|This is not every...|[this, is, not, e...|\n",
            "|    4|But in 2002, when...|[but, in, 2002,, ...|\n",
            "+-----+--------------------+--------------------+\n",
            "\n",
            "+-----+--------------------+--------------------+--------------------+--------------------+\n",
            "|label|            sentence|               words|         rawFeatures|            features|\n",
            "+-----+--------------------+--------------------+--------------------+--------------------+\n",
            "|    0|When you ask Amer...|[when, you, ask, ...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|\n",
            "|    1|So a new coach is...|[so, a, new, coac...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|\n",
            "|    2|Whether it's in t...|[whether, it's, i...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|\n",
            "|    3|This is not every...|[this, is, not, e...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|\n",
            "|    4|But in 2002, when...|[but, in, 2002,, ...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|\n",
            "+-----+--------------------+--------------------+--------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZO367gzF3oN",
        "outputId": "6e75003d-7fea-4b71-9612-2f72f4ba11b2"
      },
      "source": [
        "#for lemmitized inputs \n",
        "# itterate thru the inputs and lemitize the words \n",
        "lemmatizer = WordNetLemmatizer()\n",
        "wordset = wordsData.select('words').collect() \n",
        "for i in range(0,len(wordset)):\n",
        "  for j in range(0,len(wordset[i][0])):\n",
        "    wordset[i][0][j] = lemmatizer.lemmatize(wordset[i][0][j])\n",
        "    \n",
        "# recast list into a data frame and pass it to the TF-IDF functions \n",
        "wordset = spark.createDataFrame(wordset)\n",
        "output_2 = TF(wordset)\n",
        "output_2.select(\"words\", \"features\").show(10, truncate = True)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+\n",
            "|               words|            features|\n",
            "+--------------------+--------------------+\n",
            "|[when, you, ask, ...|(10,[0,1,2,3,4,5,...|\n",
            "|[so, a, new, coac...|(10,[0,1,2,3,4,5,...|\n",
            "|[whether, it's, i...|(10,[0,1,2,3,4,5,...|\n",
            "|[this, is, not, e...|(10,[0,1,2,3,4,5,...|\n",
            "|[but, in, 2002,, ...|(10,[0,1,2,3,4,5,...|\n",
            "+--------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jqEwRiQHbOe",
        "outputId": "368364d9-b30b-43f7-f157-a5c4965f85bc"
      },
      "source": [
        "#creating NGrams \n",
        "# output looks good for the set up \n",
        "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\n",
        "ngramDataFrame = ngram.transform(wordsData)\n",
        "ngramDataFrame.show()\n",
        "output_3 = TF(ngramDataFrame)\n",
        "output_3.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+--------------------+--------------------+\n",
            "|label|            sentence|               words|              ngrams|\n",
            "+-----+--------------------+--------------------+--------------------+\n",
            "|    0|When you ask Amer...|[when, you, ask, ...|[when you, you as...|\n",
            "|    1|So a new coach is...|[so, a, new, coac...|[so a, a new, new...|\n",
            "|    2|Whether it's in t...|[whether, it's, i...|[whether it's, it...|\n",
            "|    3|This is not every...|[this, is, not, e...|[this is, is not,...|\n",
            "|    4|But in 2002, when...|[but, in, 2002,, ...|[but in, in 2002,...|\n",
            "+-----+--------------------+--------------------+--------------------+\n",
            "\n",
            "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|label|            sentence|               words|              ngrams|         rawFeatures|            features|\n",
            "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|    0|When you ask Amer...|[when, you, ask, ...|[when you, you as...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|\n",
            "|    1|So a new coach is...|[so, a, new, coac...|[so a, a new, new...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|\n",
            "|    2|Whether it's in t...|[whether, it's, i...|[whether it's, it...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|\n",
            "|    3|This is not every...|[this, is, not, e...|[this is, is not,...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|\n",
            "|    4|But in 2002, when...|[but, in, 2002,, ...|[but in, in 2002,...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|\n",
            "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ma643WfPz9u",
        "outputId": "ff0f5870-429b-4340-daf7-184637588968"
      },
      "source": [
        "#2 word to vec \n",
        "# data set now set up with word to vector \n",
        "word2Vec = Word2Vec(vectorSize=2, minCount=0, inputCol=\"text\", outputCol=\"result\")\n",
        "model = word2Vec.fit(word_to_vec)\n",
        "result = model.transform(word_to_vec)\n",
        "print(result.show())\n",
        "# change column name so it can be passed to TF function \n",
        "result = reslut.withColumnRenamed('text', 'words')\n",
        "ouput_nonlp = TF(result)\n",
        "ouput_nonlp.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+\n",
            "|                text|              result|\n",
            "+--------------------+--------------------+\n",
            "|[When, you, ask, ...|[-0.0251492020776...|\n",
            "|[So, a, new, coac...|[0.01094523886914...|\n",
            "|[Whether, it's, i...|[0.01033639257945...|\n",
            "|[This, is, not, e...|[0.00965418570558...|\n",
            "|[But, in, 2002,, ...|[-0.0256646900093...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "None\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "|               words|              result|         rawFeatures|            features|\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "|[When, you, ask, ...|[-0.0251492020776...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|\n",
            "|[So, a, new, coac...|[0.01094523886914...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|\n",
            "|[Whether, it's, i...|[0.01033639257945...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|\n",
            "|[This, is, not, e...|[0.00965418570558...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|\n",
            "|[But, in, 2002,, ...|[-0.0256646900093...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0SeqHhRuQoe",
        "outputId": "c432f985-b58f-4ae1-81af-ce564c1d46e9"
      },
      "source": [
        "\n",
        "# lemitization for quesiton 2 part b \n",
        "# wordset two can now be used for quesiton 2 part b \n",
        "wordset2 = word_to_vec.select('text').collect() \n",
        "for i in range(0,len(wordset2)):\n",
        "    for j in range(0,len(wordset2[i][0])):\n",
        "        wordset2[i][0][j] = lemmatizer.lemmatize(wordset2[i][0][j])\n",
        "\n",
        "wordset2 = spark.createDataFrame(wordset2)\n",
        "word2Vec = Word2Vec(vectorSize=2, minCount=0, inputCol=\"text\", outputCol=\"result\")\n",
        "model = word2Vec.fit(wordset2)\n",
        "result = model.transform(wordset2)\n",
        "print(result.show(10))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+\n",
            "|                text|              result|\n",
            "+--------------------+--------------------+\n",
            "|[When, you, ask, ...|[-0.0271476942115...|\n",
            "|[So, a, new, coac...|[0.00787496337234...|\n",
            "|[Whether, it's, i...|[0.00708858389407...|\n",
            "|[This, is, not, e...|[-0.0048036088046...|\n",
            "|[But, in, 2002,, ...|[-0.0131841920429...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJMHDCiI3PpY",
        "outputId": "02e6bc8b-8461-413d-c1a8-14ce49e27254"
      },
      "source": [
        "# word vec with ngram \n",
        "#word2Vec_ngram = spark.createDataFrame(word_to_vec)\n",
        "# toke the orignal ngrams data set \n",
        "# then re ran the w2vec conde on it \n",
        "ngram = spark.createDataFrame(ngramDataFrame.select(\"ngrams\").collect())\n",
        "word2Vec = Word2Vec(vectorSize=2, minCount=0, inputCol=\"ngrams\", outputCol=\"result\")\n",
        "model = word2Vec.fit(ngramDataFrame)\n",
        "result = model.transform(ngramDataFrame)\n",
        "print(result.show())\n",
        "output_3 = TF(result)\n",
        "output_3.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+--------------------+--------------------+--------------------+\n",
            "|label|            sentence|               words|              ngrams|              result|\n",
            "+-----+--------------------+--------------------+--------------------+--------------------+\n",
            "|    0|When you ask Amer...|[when, you, ask, ...|[when you, you as...|[-0.0133773904573...|\n",
            "|    1|So a new coach is...|[so, a, new, coac...|[so a, a new, new...|[0.00359556692186...|\n",
            "|    2|Whether it's in t...|[whether, it's, i...|[whether it's, it...|[-0.0276695854587...|\n",
            "|    3|This is not every...|[this, is, not, e...|[this is, is not,...|[-0.0131260473988...|\n",
            "|    4|But in 2002, when...|[but, in, 2002,, ...|[but in, in 2002,...|[0.01039087617495...|\n",
            "+-----+--------------------+--------------------+--------------------+--------------------+\n",
            "\n",
            "None\n",
            "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|label|            sentence|               words|              ngrams|              result|         rawFeatures|            features|\n",
            "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|    0|When you ask Amer...|[when, you, ask, ...|[when you, you as...|[-0.0133773904573...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|\n",
            "|    1|So a new coach is...|[so, a, new, coac...|[so a, a new, new...|[0.00359556692186...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|\n",
            "|    2|Whether it's in t...|[whether, it's, i...|[whether it's, it...|[-0.0276695854587...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|\n",
            "|    3|This is not every...|[this, is, not, e...|[this is, is not,...|[-0.0131260473988...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|\n",
            "|    4|But in 2002, when...|[but, in, 2002,, ...|[but in, in 2002,...|[0.01039087617495...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|\n",
            "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDAF9xev64nn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}