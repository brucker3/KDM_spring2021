{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ICP5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOltD8HmwDvZIwXv3t33xxb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brucker3/KDM_spring2021/blob/main/ICP5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L68tMoXgNihH",
        "outputId": "a4ac2f2f-2f36-4878-f4ca-9035c2cdb401"
      },
      "source": [
        "# intsall spark and importing necessry packages \n",
        "!pip install pyspark\n",
        "!pip install nltk\n",
        "from __future__ import print_function\n",
        "from pyspark import SparkConf, SparkContext\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import NGram\n",
        "from pyspark.ml.feature import Word2Vec\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# creating spark session\n",
        "spark = SparkSession.builder.appName(\"ICP5\").getOrCreate()\n"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.6/dist-packages (from pyspark) (0.10.9)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXCY1T_7Zcr1"
      },
      "source": [
        "\n",
        "# read in of all 5 txt files \n",
        "# I tried to do this a cleaner way but spark kept skipping over some of the files \n",
        "\n",
        "txt1 = \"../content/txt1.txt\"\n",
        "txt2 = \"../content/txt2.txt\"\n",
        "txt3 = \"../content/txt3.txt\"\n",
        "txt4 = \"../content/txt4.txt\"\n",
        "txt5 = \"../content/txt5.txt\"\n",
        "\n",
        "# opening and converting read in files to use able form \n",
        "txt1 = open(txt1, \"r\")\n",
        "txt2 = open(txt2, \"r\")\n",
        "txt3 = open(txt3, \"r\")\n",
        "txt4 = open(txt4, \"r\")\n",
        "txt5 = open(txt5, \"r\")\n",
        "text1_sring = txt1.read()\n",
        "text2_sring = txt2.read()\n",
        "text3_sring = txt3.read()\n",
        "text4_sring = txt4.read()\n",
        "text5_sring = txt5.read()\n",
        "# creating spark dataframe wiht the input data. \n",
        "sentenceData = spark.createDataFrame([\n",
        "        (0, text1_sring),\n",
        "        (1, text2_sring),\n",
        "        (2, text3_sring),\n",
        "        (3, text4_sring),\n",
        "        (4, text5_sring)\n",
        "    ], [\"label\", \"sentence\"])\n",
        "\n",
        "\n",
        "word_to_vec = spark.createDataFrame([\n",
        "        ( text1_sring.split(\" \"),),\n",
        "        ( text2_sring.split(\" \"),),\n",
        "        ( text3_sring.split(\" \"),),\n",
        "        ( text4_sring.split(\" \"),),\n",
        "        ( text5_sring.split(\" \"),)\n",
        "    ], [\"text\"])\n",
        "\n"
      ],
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nm35o18iiABP",
        "outputId": "e9a5f0df-3d73-4c3c-f665-520510ff94c8"
      },
      "source": [
        "# tokenizer to set up for IDF calculation \n",
        "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
        "wordsData = tokenizer.transform(sentenceData)\n",
        "wordsData.show()"
      ],
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+--------------------+\n",
            "|label|            sentence|               words|\n",
            "+-----+--------------------+--------------------+\n",
            "|    0|When you ask Amer...|[when, you, ask, ...|\n",
            "|    1|So a new coach is...|[so, a, new, coac...|\n",
            "|    2|Whether it's in t...|[whether, it's, i...|\n",
            "|    3|This is not every...|[this, is, not, e...|\n",
            "|    4|But in 2002, when...|[but, in, 2002,, ...|\n",
            "+-----+--------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gF0CzIoND7Cm",
        "outputId": "7533a2ff-098d-4c12-a348-5ef6d94bc9dc"
      },
      "source": [
        "\n",
        "def TF(words):\n",
        "    # applying tf on the words data\n",
        "    # alternatively, CountVectorizer can also be used to get term frequency vectors\n",
        "    hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=10)\n",
        "    featurizedData = hashingTF.transform(words)\n",
        "    featurizedData.cache()\n",
        "    # calculating the IDF\n",
        "    idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "    idfModel = idf.fit(featurizedData)\n",
        "    rescaledData = idfModel.transform(featurizedData)\n",
        "    return rescaledData\n",
        "\n",
        "# for the orignal input \n",
        "#type(wordsData)\n",
        "wordsData.show()\n",
        "ouput = TF(wordsData)\n",
        "ouput.show()  "
      ],
      "execution_count": 302,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+--------------------+\n",
            "|label|            sentence|               words|\n",
            "+-----+--------------------+--------------------+\n",
            "|    0|When you ask Amer...|[when, you, ask, ...|\n",
            "|    1|So a new coach is...|[so, a, new, coac...|\n",
            "|    2|Whether it's in t...|[whether, it's, i...|\n",
            "|    3|This is not every...|[this, is, not, e...|\n",
            "|    4|But in 2002, when...|[but, in, 2002,, ...|\n",
            "+-----+--------------------+--------------------+\n",
            "\n",
            "+-----+--------------------+--------------------+--------------------+--------------------+\n",
            "|label|            sentence|               words|         rawFeatures|            features|\n",
            "+-----+--------------------+--------------------+--------------------+--------------------+\n",
            "|    0|When you ask Amer...|[when, you, ask, ...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|\n",
            "|    1|So a new coach is...|[so, a, new, coac...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|\n",
            "|    2|Whether it's in t...|[whether, it's, i...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|\n",
            "|    3|This is not every...|[this, is, not, e...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|\n",
            "|    4|But in 2002, when...|[but, in, 2002,, ...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|\n",
            "+-----+--------------------+--------------------+--------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "RGfCxFr2AEsk",
        "outputId": "559421d6-016b-4eb5-c88f-49219e42a380"
      },
      "source": [
        "# for the orignal input \n",
        "print(type(wordsData)\n",
        "wordsData.show()\n",
        "#ouput = TF(wordsData)\n",
        "#ouput.show()  "
      ],
      "execution_count": 294,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-294-8781c66e494b>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    TF(wordsData)\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 783
        },
        "id": "xZO367gzF3oN",
        "outputId": "ca486ab7-1f9f-4cc0-ec68-7ec1c74cf8a8"
      },
      "source": [
        "#for lemmitized inputs \n",
        "# itterate thru the inputs and lemitize the words \n",
        "lemmatizer = WordNetLemmatizer()\n",
        "wordset = wordsData.select('words').collect() \n",
        "for i in range(0,len(wordset)):\n",
        "  for j in range(0,len(wordset[i][0])):\n",
        "    wordset[i][0][j] = lemmatizer.lemmatize(wordset[i][0][j])\n",
        "    \n",
        "# recast list into a data frame and pass it to the TF-IDF functions \n",
        "wordset = spark.createDataFrame(wordset)\n",
        "output_2 = TF(wordset)\n",
        "output_2.select(\"words\", \"features\").show(10, truncate = True)\n"
      ],
      "execution_count": 289,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-289-ca1b952d48da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# recast list into a data frame and pass it to the TF-IDF functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mwordset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0moutput_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0moutput_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"words\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"features\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-286-c4fa7bb5b8b6>\u001b[0m in \u001b[0;36mTF\u001b[0;34m(words)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0midfModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeaturizedData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mrescaledData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midfModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeaturizedData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0midfIgnore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminDocFreq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeaturizedData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mtfidfIgnore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midfIgnore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeaturizedData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtfidfIgnore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \"\"\"\n\u001b[1;32m    317\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o9830.fit.\n: java.util.NoSuchElementException: Failed to find a default value for inputCol\n\tat org.apache.spark.ml.param.Params.$anonfun$getOrDefault$2(params.scala:756)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.ml.param.Params.getOrDefault(params.scala:756)\n\tat org.apache.spark.ml.param.Params.getOrDefault$(params.scala:753)\n\tat org.apache.spark.ml.PipelineStage.getOrDefault(Pipeline.scala:41)\n\tat org.apache.spark.ml.param.Params.$(params.scala:762)\n\tat org.apache.spark.ml.param.Params.$$(params.scala:762)\n\tat org.apache.spark.ml.PipelineStage.$(Pipeline.scala:41)\n\tat org.apache.spark.ml.feature.IDFBase.validateAndTransformSchema(IDF.scala:60)\n\tat org.apache.spark.ml.feature.IDFBase.validateAndTransformSchema$(IDF.scala:59)\n\tat org.apache.spark.ml.feature.IDF.validateAndTransformSchema(IDF.scala:69)\n\tat org.apache.spark.ml.feature.IDF.transformSchema(IDF.scala:99)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)\n\tat org.apache.spark.ml.feature.IDF.fit(IDF.scala:89)\n\tat jdk.internal.reflect.GeneratedMethodAccessor226.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 953
        },
        "id": "0jqEwRiQHbOe",
        "outputId": "eb5645cd-d7bc-43c1-b3c4-9cb06366bd37"
      },
      "source": [
        "#creating NGrams \n",
        "# output looks good for the set up \n",
        "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\n",
        "ngramDataFrame = ngram.transform(wordsData)\n",
        "ngramDataFrame.show()\n",
        "output_3 = TF(ngramDataFrame)\n",
        "output_3.show()"
      ],
      "execution_count": 290,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+--------------------+--------------------+\n",
            "|label|            sentence|               words|              ngrams|\n",
            "+-----+--------------------+--------------------+--------------------+\n",
            "|    0|When you ask Amer...|[when, you, ask, ...|[when you, you as...|\n",
            "|    1|So a new coach is...|[so, a, new, coac...|[so a, a new, new...|\n",
            "|    2|Whether it's in t...|[whether, it's, i...|[whether it's, it...|\n",
            "|    3|This is not every...|[this, is, not, e...|[this is, is not,...|\n",
            "|    4|But in 2002, when...|[but, in, 2002,, ...|[but in, in 2002,...|\n",
            "+-----+--------------------+--------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-290-c568f137e7ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mngramDataFrame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngram\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordsData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mngramDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0moutput_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngramDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0moutput_3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-286-c4fa7bb5b8b6>\u001b[0m in \u001b[0;36mTF\u001b[0;34m(words)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0midfModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeaturizedData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mrescaledData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midfModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeaturizedData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0midfIgnore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminDocFreq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeaturizedData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mtfidfIgnore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midfIgnore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeaturizedData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtfidfIgnore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \"\"\"\n\u001b[1;32m    317\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o9914.fit.\n: java.util.NoSuchElementException: Failed to find a default value for inputCol\n\tat org.apache.spark.ml.param.Params.$anonfun$getOrDefault$2(params.scala:756)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.ml.param.Params.getOrDefault(params.scala:756)\n\tat org.apache.spark.ml.param.Params.getOrDefault$(params.scala:753)\n\tat org.apache.spark.ml.PipelineStage.getOrDefault(Pipeline.scala:41)\n\tat org.apache.spark.ml.param.Params.$(params.scala:762)\n\tat org.apache.spark.ml.param.Params.$$(params.scala:762)\n\tat org.apache.spark.ml.PipelineStage.$(Pipeline.scala:41)\n\tat org.apache.spark.ml.feature.IDFBase.validateAndTransformSchema(IDF.scala:60)\n\tat org.apache.spark.ml.feature.IDFBase.validateAndTransformSchema$(IDF.scala:59)\n\tat org.apache.spark.ml.feature.IDF.validateAndTransformSchema(IDF.scala:69)\n\tat org.apache.spark.ml.feature.IDF.transformSchema(IDF.scala:99)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)\n\tat org.apache.spark.ml.feature.IDF.fit(IDF.scala:89)\n\tat jdk.internal.reflect.GeneratedMethodAccessor226.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ma643WfPz9u",
        "outputId": "ff0f5870-429b-4340-daf7-184637588968"
      },
      "source": [
        "#2 word to vec \n",
        "# data set now set up with word to vector \n",
        "word2Vec = Word2Vec(vectorSize=2, minCount=0, inputCol=\"text\", outputCol=\"result\")\n",
        "model = word2Vec.fit(word_to_vec)\n",
        "result = model.transform(word_to_vec)\n",
        "print(result.show())\n",
        "# change column name so it can be passed to TF function \n",
        "result = reslut.withColumnRenamed('text', 'words')\n",
        "ouput_nonlp = TF(result)\n",
        "ouput_nonlp.show()\n",
        "\n"
      ],
      "execution_count": 279,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+\n",
            "|                text|              result|\n",
            "+--------------------+--------------------+\n",
            "|[When, you, ask, ...|[-0.0251492020776...|\n",
            "|[So, a, new, coac...|[0.01094523886914...|\n",
            "|[Whether, it's, i...|[0.01033639257945...|\n",
            "|[This, is, not, e...|[0.00965418570558...|\n",
            "|[But, in, 2002,, ...|[-0.0256646900093...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "None\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "|               words|              result|         rawFeatures|            features|\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "|[When, you, ask, ...|[-0.0251492020776...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|\n",
            "|[So, a, new, coac...|[0.01094523886914...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|\n",
            "|[Whether, it's, i...|[0.01033639257945...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|\n",
            "|[This, is, not, e...|[0.00965418570558...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|\n",
            "|[But, in, 2002,, ...|[-0.0256646900093...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0SeqHhRuQoe",
        "outputId": "c432f985-b58f-4ae1-81af-ce564c1d46e9"
      },
      "source": [
        "\n",
        "# lemitization for quesiton 2 part b \n",
        "# wordset two can now be used for quesiton 2 part b \n",
        "wordset2 = word_to_vec.select('text').collect() \n",
        "for i in range(0,len(wordset2)):\n",
        "    for j in range(0,len(wordset2[i][0])):\n",
        "        wordset2[i][0][j] = lemmatizer.lemmatize(wordset2[i][0][j])\n",
        "\n",
        "wordset2 = spark.createDataFrame(wordset2)\n",
        "word2Vec = Word2Vec(vectorSize=2, minCount=0, inputCol=\"text\", outputCol=\"result\")\n",
        "model = word2Vec.fit(wordset2)\n",
        "result = model.transform(wordset2)\n",
        "print(result.show(10))\n"
      ],
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+\n",
            "|                text|              result|\n",
            "+--------------------+--------------------+\n",
            "|[When, you, ask, ...|[-0.0271476942115...|\n",
            "|[So, a, new, coac...|[0.00787496337234...|\n",
            "|[Whether, it's, i...|[0.00708858389407...|\n",
            "|[This, is, not, e...|[-0.0048036088046...|\n",
            "|[But, in, 2002,, ...|[-0.0131841920429...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJMHDCiI3PpY",
        "outputId": "02e6bc8b-8461-413d-c1a8-14ce49e27254"
      },
      "source": [
        "# word vec with ngram \n",
        "#word2Vec_ngram = spark.createDataFrame(word_to_vec)\n",
        "# toke the orignal ngrams data set \n",
        "# then re ran the w2vec conde on it \n",
        "ngram = spark.createDataFrame(ngramDataFrame.select(\"ngrams\").collect())\n",
        "word2Vec = Word2Vec(vectorSize=2, minCount=0, inputCol=\"ngrams\", outputCol=\"result\")\n",
        "model = word2Vec.fit(ngramDataFrame)\n",
        "result = model.transform(ngramDataFrame)\n",
        "print(result.show())\n",
        "output_3 = TF(result)\n",
        "output_3.show()\n",
        "\n"
      ],
      "execution_count": 273,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+--------------------+--------------------+--------------------+\n",
            "|label|            sentence|               words|              ngrams|              result|\n",
            "+-----+--------------------+--------------------+--------------------+--------------------+\n",
            "|    0|When you ask Amer...|[when, you, ask, ...|[when you, you as...|[-0.0133773904573...|\n",
            "|    1|So a new coach is...|[so, a, new, coac...|[so a, a new, new...|[0.00359556692186...|\n",
            "|    2|Whether it's in t...|[whether, it's, i...|[whether it's, it...|[-0.0276695854587...|\n",
            "|    3|This is not every...|[this, is, not, e...|[this is, is not,...|[-0.0131260473988...|\n",
            "|    4|But in 2002, when...|[but, in, 2002,, ...|[but in, in 2002,...|[0.01039087617495...|\n",
            "+-----+--------------------+--------------------+--------------------+--------------------+\n",
            "\n",
            "None\n",
            "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|label|            sentence|               words|              ngrams|              result|         rawFeatures|            features|\n",
            "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|    0|When you ask Amer...|[when, you, ask, ...|[when you, you as...|[-0.0133773904573...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|\n",
            "|    1|So a new coach is...|[so, a, new, coac...|[so a, a new, new...|[0.00359556692186...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|\n",
            "|    2|Whether it's in t...|[whether, it's, i...|[whether it's, it...|[-0.0276695854587...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|\n",
            "|    3|This is not every...|[this, is, not, e...|[this is, is not,...|[-0.0131260473988...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|\n",
            "|    4|But in 2002, when...|[but, in, 2002,, ...|[but in, in 2002,...|[0.01039087617495...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|\n",
            "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDAF9xev64nn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}